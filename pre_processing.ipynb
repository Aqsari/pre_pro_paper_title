{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title  \\\n",
      "0  I enjoy writing and playing, do you?: a person...   \n",
      "1  Teaching students about conversational ai usin...   \n",
      "2  Towards an online empathetic chatbot with emot...   \n",
      "3  Compeer: A generative conversational agent for...   \n",
      "4  Conversational AI-Chatbot Architectures and Ev...   \n",
      "\n",
      "                                                Link  Year  \\\n",
      "0  https://ieeexplore.ieee.org/abstract/document/...  2022   \n",
      "1  https://ieeexplore.ieee.org/abstract/document/...  2021   \n",
      "2  https://dl.acm.org/doi/abs/10.1145/3404835.346...  2021   \n",
      "3  https://dl.acm.org/doi/abs/10.1145/3654777.367...  2024   \n",
      "4  https://sydneyacademics.com/index.php/ajmlra/a...  2021   \n",
      "\n",
      "                                            Abstract  \n",
      "0  conversational agent to communicate with its u...  \n",
      "1  CONVO to train ML models and create conversati...  \n",
      "2  To answer these sub-questions, we develop an E...  \n",
      "3  generative agents to improve people‚Äôs mental h...  \n",
      "4  studies, to assess the quality of conversation...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Nama file CSV\n",
    "csv_filename = \"scholar_titles_links.csv\"\n",
    "\n",
    "# Membaca file CSV\n",
    "try:\n",
    "    df = pd.read_csv(csv_filename, encoding=\"utf-8\")\n",
    "    print(df.head())  # Menampilkan 5 baris pertama\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå File '{csv_filename}' tidak ditemukan.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Terjadi kesalahan: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowercasing: Mengubah teks menjadi huruf kecil.\n",
    "\n",
    "Removing Punctuation & Special Characters: Menghapus tanda baca.\n",
    "\n",
    "Removing Numbers: Menghapus angka dari teks.\n",
    "\n",
    "Tokenization: Memisahkan teks menjadi kata-kata.\n",
    "\n",
    "Stopword Removal: Menghapus kata-kata umum yang tidak memiliki makna penting.\n",
    "\n",
    "Lemmatization: Mengubah kata menjadi bentuk dasarnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All necessary NLTK resources have been downloaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Unity_Comp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Unity_Comp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Unity_Comp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Unity_Comp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the required resources again\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "print(\"‚úÖ All necessary NLTK resources have been downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Inisialisasi Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Fungsi untuk membersihkan teks (hanya untuk kolom Title dan Abstract).\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # Join back into a string\n",
    "    cleaned_text = \" \".join(tokens)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Preprocessing selesai! Data disimpan ke 'cleaned_titles.csv'.\n",
      "                                               Title\n",
      "0  enjoy writing playing personalized emotion gro...\n",
      "1  teaching student conversational ai using convo...\n",
      "2    towards online empathetic chatbot emotion cause\n",
      "3  compeer generative conversational agent proact...\n",
      "4  conversational aichatbot architecture evaluati...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Bersihkan hanya kolom \"Title\"\n",
    "    df[\"Title\"] = df[\"Title\"].apply(clean_text)\n",
    "\n",
    "    # Simpan kembali hanya kolom Title ke CSV\n",
    "    cleaned_csv_filename = \"cleaned_titles.csv\"\n",
    "    df[[\"Title\"]].to_csv(cleaned_csv_filename, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Preprocessing selesai! Data disimpan ke '{cleaned_csv_filename}'.\")\n",
    "    df = pd.read_csv(cleaned_csv_filename, encoding=\"utf-8\")\n",
    "    print(df.head())  # Tampilkan beberapa hasil pertama\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå File '{cleaned_csv_filename}' tidak ditemukan.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Terjadi kesalahan: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä 20 Kata Paling Sering Muncul:\n",
      "conversational: 283\n",
      "agent: 226\n",
      "generative: 96\n",
      "chatbot: 70\n",
      "ai: 59\n",
      "using: 38\n",
      "chat: 32\n",
      "learning: 31\n",
      "model: 31\n",
      "artificial: 30\n",
      "intelligence: 28\n",
      "health: 27\n",
      "emotion: 25\n",
      "study: 25\n",
      "chatgpt: 25\n",
      "emotional: 24\n",
      "system: 23\n",
      "interaction: 23\n",
      "user: 23\n",
      "support: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Unity_Comp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Download tokenizer jika belum ada\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# üìÇ Baca file CSV\n",
    "csv_filename = \"cleaned_titles.csv\"\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(csv_filename, encoding=\"utf-8\")\n",
    "\n",
    "    # Pastikan kolom \"Title\" ada dalam dataset\n",
    "    if \"Title\" not in df.columns:\n",
    "        raise ValueError(\"‚ùå Kolom 'Title' tidak ditemukan dalam dataset.\")\n",
    "\n",
    "    # Gabungkan semua teks di kolom \"Title\"\n",
    "    combined_text = \" \".join(df[\"Title\"].astype(str))\n",
    "\n",
    "    # Tokenisasi teks (memisahkan kata-kata)\n",
    "    words = nltk.word_tokenize(combined_text)\n",
    "\n",
    "    # Hitung frekuensi kata\n",
    "    freq_dist = FreqDist(words)\n",
    "\n",
    "    # Ambil 20 kata paling sering muncul\n",
    "    most_common_words = freq_dist.most_common(20)\n",
    "\n",
    "    print(\"\\nüìä 20 Kata Paling Sering Muncul:\")\n",
    "    for word, count in most_common_words:\n",
    "        print(f\"{word}: {count}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå File '{csv_filename}' tidak ditemukan.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Terjadi kesalahan: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Unity_Comp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Unity_Comp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Unity_Comp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Unity_Comp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\Unity_Comp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Named Entity Recognition selesai! Hasil disimpan di 'ner_output_nltk.csv'.\n",
      "                                               Title Named_Entities\n",
      "0  enjoy writing playing personalized emotion gro...             []\n",
      "1  teaching student conversational ai using convo...             []\n",
      "2    towards online empathetic chatbot emotion cause             []\n",
      "3  compeer generative conversational agent proact...             []\n",
      "4  conversational aichatbot architecture evaluati...             []\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "# Unduh resource yang dibutuhkan jika belum tersedia\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"maxent_ne_chunker\")\n",
    "nltk.download(\"words\")\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "\n",
    "# üìÇ Baca file CSV\n",
    "csv_filename = \"cleaned_titles.csv\"\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(csv_filename, encoding=\"utf-8\")\n",
    "\n",
    "    # Pastikan kolom \"Title\" ada\n",
    "    if \"Title\" not in df.columns:\n",
    "        raise ValueError(\"Kolom 'Title' tidak ditemukan dalam CSV!\")\n",
    "\n",
    "    # üîç Proses Named Entity Recognition (NER)\n",
    "    entities_list = []\n",
    "\n",
    "    for title in df[\"Title\"].astype(str):\n",
    "        tokens = word_tokenize(title)  # Tokenisasi\n",
    "        pos_tags = pos_tag(tokens)  # Part-of-Speech tagging\n",
    "        ner_tree = ne_chunk(pos_tags)  # Named Entity Recognition\n",
    "        entities = [(chunk[0], chunk.label()) for chunk in ner_tree if hasattr(chunk, 'label')]\n",
    "        entities_list.append(entities)\n",
    "\n",
    "    # Tambahkan hasil NER ke dataframe\n",
    "    df[\"Named_Entities\"] = entities_list\n",
    "\n",
    "    # üìå Simpan hasil ke file CSV baru\n",
    "    ner_output_filename = \"ner_output_nltk.csv\"\n",
    "    df.to_csv(ner_output_filename, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Named Entity Recognition selesai! Hasil disimpan di '{ner_output_filename}'.\")\n",
    "    print(df[[\"Title\", \"Named_Entities\"]].head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå File '{csv_filename}' tidak ditemukan.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Terjadi kesalahan: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Proses TF-IDF selesai! Hasil disimpan di 'tfidf_output.csv'.\n",
      "                                               Title  ability  abstract  \\\n",
      "0  enjoy writing playing personalized emotion gro...      0.0       0.0   \n",
      "1  teaching student conversational ai using convo...      0.0       0.0   \n",
      "2    towards online empathetic chatbot emotion cause      0.0       0.0   \n",
      "3  compeer generative conversational agent proact...      0.0       0.0   \n",
      "4  conversational aichatbot architecture evaluati...      0.0       0.0   \n",
      "\n",
      "   abuse  academic  acceleration  acceptability  acceptance  accounting  \\\n",
      "0    0.0       0.0           0.0            0.0         0.0         0.0   \n",
      "1    0.0       0.0           0.0            0.0         0.0         0.0   \n",
      "2    0.0       0.0           0.0            0.0         0.0         0.0   \n",
      "3    0.0       0.0           0.0            0.0         0.0         0.0   \n",
      "4    0.0       0.0           0.0            0.0         0.0         0.0   \n",
      "\n",
      "   accuracy  ...  worker  workplace  write   writing  writingtolearn  written  \\\n",
      "0       0.0  ...     0.0        0.0    0.0  0.374895             0.0      0.0   \n",
      "1       0.0  ...     0.0        0.0    0.0  0.000000             0.0      0.0   \n",
      "2       0.0  ...     0.0        0.0    0.0  0.000000             0.0      0.0   \n",
      "3       0.0  ...     0.0        0.0    0.0  0.000000             0.0      0.0   \n",
      "4       0.0  ...     0.0        0.0    0.0  0.000000             0.0      0.0   \n",
      "\n",
      "   wysa  young  youobservers  zhorai  \n",
      "0   0.0    0.0           0.0     0.0  \n",
      "1   0.0    0.0           0.0     0.0  \n",
      "2   0.0    0.0           0.0     0.0  \n",
      "3   0.0    0.0           0.0     0.0  \n",
      "4   0.0    0.0           0.0     0.0  \n",
      "\n",
      "[5 rows x 1264 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# üìÇ Baca file CSV\n",
    "csv_filename = \"cleaned_titles.csv\"\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(csv_filename, encoding=\"utf-8\")\n",
    "\n",
    "    # Pastikan kolom \"Title\" ada\n",
    "    if \"Title\" not in df.columns:\n",
    "        raise ValueError(\"Kolom 'Title' tidak ditemukan dalam CSV!\")\n",
    "\n",
    "    # üîç Proses TF-IDF\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(df[\"Title\"].astype(str))\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "    # Tambahkan hasil TF-IDF ke dataframe\n",
    "    df = pd.concat([df, tfidf_df], axis=1)\n",
    "\n",
    "    # üìå Simpan hasil ke file CSV baru\n",
    "    tfidf_output_filename = \"tfidf_output.csv\"\n",
    "    df.to_csv(tfidf_output_filename, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Proses TF-IDF selesai! Hasil disimpan di '{tfidf_output_filename}'.\")\n",
    "    print(df.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå File '{csv_filename}' tidak ditemukan.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Terjadi kesalahan: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "\n",
    "# üìÇ Baca file CSV\n",
    "csv_filename = \"cleaned_titles.csv\"\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(csv_filename, encoding=\"utf-8\")\n",
    "\n",
    "    # Pastikan kolom \"Title\" ada\n",
    "    if \"Title\" not in df.columns:\n",
    "        raise ValueError(\"Kolom 'Title' tidak ditemukan dalam CSV!\")\n",
    "\n",
    "    # üîπ Load pre-trained GloVe model\n",
    "    glove_model = api.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "    def get_glove_vector(text):\n",
    "        words = text.split()\n",
    "        word_vectors = [glove_model[word] for word in words if word in glove_model]\n",
    "        if word_vectors:\n",
    "            return np.mean(word_vectors, axis=0)\n",
    "        else:\n",
    "            return np.zeros(50)  # Ukuran vektor GloVe 50 dimensi\n",
    "\n",
    "    # üîç Proses embedding dengan GloVe\n",
    "    df[\"GloVe_Vector\"] = df[\"Title\"].astype(str).apply(get_glove_vector)\n",
    "\n",
    "    # üìå Simpan hasil ke file CSV baru\n",
    "    glove_output_filename = \"glove_output.csv\"\n",
    "    df.to_csv(glove_output_filename, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Proses embedding dengan GloVe selesai! Hasil disimpan di '{glove_output_filename}'.\")\n",
    "    print(df.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå File '{csv_filename}' tidak ditemukan.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Terjadi kesalahan: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypython3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
