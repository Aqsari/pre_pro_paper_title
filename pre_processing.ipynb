{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title  \\\n",
      "0  I enjoy writing and playing, do you?: a person...   \n",
      "1  Teaching students about conversational ai usin...   \n",
      "2  Towards an online empathetic chatbot with emot...   \n",
      "3  Compeer: A generative conversational agent for...   \n",
      "4  Conversational AI-Chatbot Architectures and Ev...   \n",
      "\n",
      "                                                Link  Year  \\\n",
      "0  https://ieeexplore.ieee.org/abstract/document/...  2022   \n",
      "1  https://ieeexplore.ieee.org/abstract/document/...  2021   \n",
      "2  https://dl.acm.org/doi/abs/10.1145/3404835.346...  2021   \n",
      "3  https://dl.acm.org/doi/abs/10.1145/3654777.367...  2024   \n",
      "4  https://sydneyacademics.com/index.php/ajmlra/a...  2021   \n",
      "\n",
      "                                            Abstract  \n",
      "0  conversational agent to communicate with its u...  \n",
      "1  CONVO to train ML models and create conversati...  \n",
      "2  To answer these sub-questions, we develop an E...  \n",
      "3  generative agents to improve people‚Äôs mental h...  \n",
      "4  studies, to assess the quality of conversation...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Nama file CSV\n",
    "csv_filename = \"scholar_titles_links.csv\"\n",
    "\n",
    "# Membaca file CSV\n",
    "try:\n",
    "    df = pd.read_csv(csv_filename, encoding=\"utf-8\")\n",
    "    print(df.head())  # Menampilkan 5 baris pertama\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå File '{csv_filename}' tidak ditemukan.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Terjadi kesalahan: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowercasing: Mengubah teks menjadi huruf kecil.\n",
    "Removing Punctuation & Special Characters: Menghapus tanda baca.\n",
    "Removing Numbers: Menghapus angka dari teks.\n",
    "Tokenization: Memisahkan teks menjadi kata-kata.\n",
    "Stopword Removal: Menghapus kata-kata umum yang tidak memiliki makna penting.\n",
    "Lemmatization: Mengubah kata menjadi bentuk dasarnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All necessary NLTK resources have been downloaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Unity_Comp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Unity_Comp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Unity_Comp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Unity_Comp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the required resources again\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "print(\"‚úÖ All necessary NLTK resources have been downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Inisialisasi Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Fungsi untuk membersihkan teks (hanya untuk kolom Title dan Abstract).\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # Join back into a string\n",
    "    cleaned_text = \" \".join(tokens)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Preprocessing selesai! Data disimpan ke 'cleaned_scholar_titles_links.csv'.\n",
      "                                               Title  \\\n",
      "0  enjoy writing playing personalized emotion gro...   \n",
      "1  teaching student conversational ai using convo...   \n",
      "2    towards online empathetic chatbot emotion cause   \n",
      "3  compeer generative conversational agent proact...   \n",
      "4  conversational aichatbot architecture evaluati...   \n",
      "\n",
      "                                                Link  Year  \\\n",
      "0  https://ieeexplore.ieee.org/abstract/document/...  2022   \n",
      "1  https://ieeexplore.ieee.org/abstract/document/...  2021   \n",
      "2  https://dl.acm.org/doi/abs/10.1145/3404835.346...  2021   \n",
      "3  https://dl.acm.org/doi/abs/10.1145/3654777.367...  2024   \n",
      "4  https://sydneyacademics.com/index.php/ajmlra/a...  2021   \n",
      "\n",
      "                                            Abstract  \n",
      "0  conversational agent communicate user assist u...  \n",
      "1  convo train ml model create conversational app...  \n",
      "2  answer subquestions develop empathetic chatbot...  \n",
      "3  generative agent improve people ‚Äô mental healt...  \n",
      "4  study assess quality conversational ai system ...  \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "\n",
    "    # Bersihkan hanya kolom \"Title\" dan \"Abstract\"\n",
    "    df[\"Title\"] = df[\"Title\"].apply(clean_text)\n",
    "    df[\"Abstract\"] = df[\"Abstract\"].apply(clean_text)\n",
    "\n",
    "    # Simpan kembali ke CSV\n",
    "    cleaned_csv_filename = \"cleaned_scholar_titles_links.csv\"\n",
    "    df.to_csv(cleaned_csv_filename, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Preprocessing selesai! Data disimpan ke '{cleaned_csv_filename}'.\")\n",
    "    print(df.head())  # Tampilkan beberapa hasil pertama\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå File '{csv_filename}' tidak ditemukan.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Terjadi kesalahan: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstring\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# üìÇ Baca file CSV\n",
    "csv_filename = \"cleaned_scholar_titles_links.csv\"\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(csv_filename, encoding=\"utf-8\")\n",
    "\n",
    "    # Bersihkan hanya kolom \"Title\" dan \"Abstract\"\n",
    "    df[\"Title\"] = df[\"Title\"].astype(str).apply(clean_text)\n",
    "    df[\"Abstract\"] = df[\"Abstract\"].astype(str).apply(clean_text)\n",
    "\n",
    "    # Gabungkan teks dari kolom Title & Abstract\n",
    "    combined_text = \" \".join(df[\"Title\"]) + \" \" + \" \".join(df[\"Abstract\"])\n",
    "\n",
    "    # Hitung frekuensi kata\n",
    "    word_counts = Counter(combined_text.split())\n",
    "\n",
    "    # Ambil 10 kata paling sering muncul\n",
    "    most_common_words = word_counts.most_common(10)\n",
    "\n",
    "    print(\"\\nüìä 10 Kata Paling Sering Muncul:\")\n",
    "    for word, count in most_common_words:\n",
    "        print(f\"{word}: {count}\")\n",
    "\n",
    "    # üìå Buat WordCloud\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(combined_text)\n",
    "\n",
    "    # Tampilkan WordCloud\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Word Cloud - Frekuensi Kata\", fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå File '{csv_filename}' tidak ditemukan.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Terjadi kesalahan: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypython3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
